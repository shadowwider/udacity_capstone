{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实施步骤：\n",
    "+ 解压缩文件，读取每个文件构建可以使用的训练集测试机\n",
    "+ 训练doc2vec构建模型\n",
    "+ 对每个文档运行doc2vec 转成向量\n",
    "+ 构建神经网络分类算法\n",
    "+ 训练分类，之后调参验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen, URLError, HTTPError\n",
    "import os\n",
    "save_path= './'\n",
    "url = 'http://www.qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz'\n",
    "f = urlopen(url,timeout=1000)\n",
    "print \"downloading \" + url\n",
    "\n",
    "        # Open our local file for writing\n",
    "with open(os.path.basename(url), \"wb\") as local_file:\n",
    "    local_file.write(f.read())\n",
    "    print \"finish\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "import helper\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#helper.download_extract(data_dir) #下载并解压文档 同时将文档数据存储到params.p中去 以便于调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sentences,collect = helper.read_file() # 将所有文档读入对象并统计每一类个数\n",
    "\n",
    "print collect\n",
    "print(sentences['20news-bydate-train'][:2])\n",
    "print(sentences['20news-bydate-test'][:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = helper.load_params() #获取数据集\n",
    "#print raw_data['20news-bydate-train'][0]\n",
    "#print raw_data['20news-bydate-test'][0]\n",
    "\n",
    "# 绘制文档数量柱状图 \n",
    "X_label = []\n",
    "Y1 = []\n",
    "Y2 = []\n",
    "for i,v in collect['20news-bydate-train'].items():\n",
    "    X_label.append(i)\n",
    "    Y1.append(v)\n",
    "for i,v in collect['20news-bydate-test'].items():   \n",
    "    Y2.append(v)\n",
    "X=np.arange(20)\n",
    "#print X\n",
    "#print Y1\n",
    "plt.xticks(X, X_label)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.bar(X,Y1,width = 0.35,facecolor = 'lightskyblue',edgecolor = 'white',label= 'train')\n",
    "#width:柱的宽度\n",
    "plt.bar(X+0.35,Y2,width = 0.35,facecolor = 'yellowgreen',edgecolor = 'white',label= 'test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#统计\n",
    "train_no = len(raw_data['20news-bydate-train'])\n",
    "test_no = len(raw_data['20news-bydate-test'])\n",
    "total_news = train_no + test_no\n",
    "print('训练集共有新闻：%s\\n测试集共有新闻：%s\\n新闻总数：%s\\n '%(train_no,test_no, total_news ))\n",
    "# 统计每一类\n",
    "from collections import Counter\n",
    "total_counts = Counter()\n",
    "word_total_each = []\n",
    "for type in raw_data:    \n",
    "    for news in raw_data[type]:\n",
    "        i = 0\n",
    "        for word in news[0].split(\" \"):\n",
    "            total_counts[word] += 1  \n",
    "            i += 1\n",
    "        word_total_each.append(i)\n",
    "word_total_each = np.array(word_total_each)\n",
    "print total_counts.most_common(100) #获取频率最高的单词\n",
    "print sum(total_counts.values()) #获取单词总数\n",
    "print len(total_counts) #获取单词种类\n",
    "print len(word_total_each)\n",
    "print word_total_each[:10]\n",
    "print np.median(word_total_each),np.mean(word_total_each),np.std(word_total_each),np.amax(word_total_each),np.amin(word_total_each),np.percentile(word_total_each, 25),np.percentile(word_total_each, 75)\n",
    "print total_counts.most_common()[:-100:-1] # 获取频率最低的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import random\n",
    "from random import shuffle\n",
    "import datetime\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from collections import OrderedDict\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次错误的实现代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import gensim\n",
    "import random\n",
    "from random import shuffle\n",
    "import datetime\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "x_train,y_train = helper.labeled_data(raw_data['20news-bydate-train'])\n",
    "x_test,y_test = helper.labeled_data(raw_data['20news-bydate-test'])\n",
    "\n",
    "#将news转化为doc2vec model可以处理的对象\n",
    "def labelizeDoc(doc, label,y):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(doc):\n",
    "        #str1 = ''.join(str(e) for e in y[i])\n",
    "        tag = '%s_%s_%s'%(label,i,y[i])\n",
    "        labelized.append(LabeledSentence(v,[tag]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeDoc(x_train, 'TRAIN',y_train)\n",
    "x_test = labelizeDoc(x_test, 'TEST',y_test)\n",
    "print y_train\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "size = 100\n",
    "min_count = 1\n",
    "window=10\n",
    "negative=5\n",
    "dm=0\n",
    "\n",
    "#构建两种模型融合的向量\n",
    "model_dm = gensim.models.Doc2Vec(min_count=min_count, window=window, size=size, sample=1e-3, negative=negative, workers=4)\n",
    "\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=min_count, window=window, size=size, sample=1e-3, negative=negative, dm=dm, workers=4)\n",
    "#model_dm = gensim.models.Doc2Vec.load('db.pkl')\n",
    "#model_dbow = gensim.models.Doc2Vec.load('dm.pkl')\n",
    "listall = x_train + x_test\n",
    "l = len(listall)\n",
    "\n",
    "model_dm.build_vocab(listall)\n",
    "model_dbow.build_vocab(listall)\n",
    "\n",
    "\n",
    "\n",
    "model_dm.train(listall,total_examples=model_dm.corpus_count, epochs=20)\n",
    "model_dbow.train(listall,total_examples=model_dbow.corpus_count, epochs=20)\n",
    "\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.infer_vector(corpus[z].words)).reshape((1,size)) for z in range(l)]\n",
    "    return np.concatenate(vecs)\n",
    "\n",
    "\n",
    "\n",
    "train_vecs_dm = getVecs(model_dm,listall,size)\n",
    "train_vecs_dbow = getVecs(model_dbow,listall,size)\n",
    "model_dbow.save('db.pkl')\n",
    "model_dm.save('dm.pkl')\n",
    "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "\n",
    "#print listall\n",
    "#pickle.dump(train_vecs, open('vec.p', 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正确的代码开始 一种新的方法显示文档转向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义函数将普通文档转换维doc2vec可以接受的 TaggedDocument类\n",
    "def read_corpus(data,typename,idently):    \n",
    "    i = 0\n",
    "    for i, v in enumerate(data):       \n",
    "        tagname = typename + '_'+str(i)+'_'+ str(idently[i])  \n",
    "        #不用去除停止词 更准确\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(v[0]), [tagname])         \n",
    "        #yield gensim.models.doc2vec.TaggedDocument([token for token in gensim.utils.simple_preprocess(v[0]) if token not in STOPWORDS], [tagname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "根据官方示例 需要多次训练并且每次训练打乱原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取训练集和测试集数据\n",
    "x_train,y_train = helper.labeled_data(raw_data['20news-bydate-train'])\n",
    "x_test,y_test = helper.labeled_data(raw_data['20news-bydate-test'])\n",
    "#因为训练词向量 所以使用所有数据而不是仅仅测试集\n",
    "#x_train += x_test\n",
    "#y_train += y_test  #这里不再将测试集加入到词向量的训练中去\n",
    "\n",
    "train_corpus = list(read_corpus(x_train, 'TRAIN',y_train))\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "#最佳模型采用 PV-DBOW ，size表示最后输出维度，min_count 表示多大重复的字才会被计算到词典中，这里使用2是增加多次出现词的权重\n",
    "simple_models = [    \n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    #Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2),\n",
    "    # PV-DM w/average\n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2),\n",
    "]\n",
    "simple_models[0].build_vocab(train_corpus) # 词典只用建立一次\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "for epoch in range(passes):\n",
    "   \n",
    "    shuffle(train_corpus)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "       \n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha  # 随着训练不断减小alpha\n",
    "        \n",
    "        # 模型训练 train_model.iter 未定义时默认是5 ，所以20次迭代实际一共训练 5*20= 100 次\n",
    "        train_model.train(train_corpus, total_examples=train_model.corpus_count, epochs=train_model.iter) \n",
    "    \n",
    "    if ((epoch + 1) % 5) == 0 or epoch == 0: \n",
    "        print 'epoch %s finish'%epoch\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))\n",
    "\n",
    "#储存doc2vec的model\n",
    "train_model.save('docvec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型 把备选类别中的文档转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model = Doc2Vec.load('best')\n",
    "#获取这四类新闻的文本和label\n",
    "x_train,y_train = helper.choiced_data(raw_data['20news-bydate-train'])\n",
    "x_test,y_test = helper.choiced_data(raw_data['20news-bydate-test'])\n",
    "\n",
    "#将文本转化为文档向量\n",
    "x_train_vec = []\n",
    "x_test_vec = []\n",
    "for i in x_train:\n",
    "    # 不去除停止词 \n",
    "    #inferred_vector = train_model.infer_vector([token for token in gensim.utils.simple_preprocess(i[0]) if token not in STOPWORDS])\n",
    "    inferred_vector = train_model.infer_vector(gensim.utils.simple_preprocess(i[0])) #这里注意输入必须是数组 而不是文本\n",
    "    x_train_vec.append(inferred_vector)\n",
    "for j in x_test:\n",
    "    #inferred_vector_test = train_model.infer_vector([token for token in gensim.utils.simple_preprocess(j[0]) if token not in STOPWORDS])\n",
    "    inferred_vector_test = train_model.infer_vector(gensim.utils.simple_preprocess(j[0]))\n",
    "    x_test_vec.append(inferred_vector_test)\n",
    "\n",
    "print x_train_vec[0],y_train[0]\n",
    "print x_test_vec[0],y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分组向量 画图可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 不同的类别在图上给予不同的颜色 对四个类别的训练集做展示\n",
    "from sklearn.manifold import TSNE\n",
    "ts = TSNE(2)\n",
    "reduced_vecs = ts.fit_transform(np.concatenate((x_train_vec[:593],x_train_vec[593:1192],x_train_vec[1192:1783],x_train_vec[1783:])))\n",
    "print reduced_vecs.shape\n",
    "for i in range(len(reduced_vecs)):\n",
    "    if i < 593:\n",
    "        color = 'b'\n",
    "    elif i>=593 and i<1192:\n",
    "        color = '#d02626'\n",
    "    elif i>=1192 and i<1783:\n",
    "        color = 'g'\n",
    "    elif i>=1783:\n",
    "        color = 'y'\n",
    "    plt.plot(reduced_vecs[i,0],reduced_vecs[i,1],marker='o',color=color,markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建神经网络分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 因为算法有设置自动停止 所以步长设置的比较小，迭代次数可以放的多\n",
    "model =  MLPClassifier(hidden_layer_sizes=90,max_iter=5000,random_state=0,verbose=True,alpha=0.001,learning_rate_init=0.0001)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train_vec)\n",
    "\n",
    "# 对得到的向量进行统一的标准化处理\n",
    "x_train_vec = scaler.transform(x_train_vec)\n",
    "x_test_vec = scaler.transform(x_test_vec)\n",
    "#print y_train\n",
    "model.fit(x_train_vec,y_train)\n",
    "pp = model.predict(x_test_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# 展示一些统计结果 验证准确度\n",
    "print(confusion_matrix(y_test,pp))\n",
    "print(classification_report(y_test,pp))\n",
    "\n",
    "\n",
    "score = model.score(x_test_vec, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网格搜索获取最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[90,70,40], 'alpha':[1, 10, 0.01, 0.001],'learning_rate_init':[0.001,0.01,0.0001]}\n",
    "model =  MLPClassifier(max_iter=1000,random_state=0)\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(x_train_vec,y_train)\n",
    "print clf.cv_results_\n",
    "print clf.best_params_\n",
    "print clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用朴素贝叶斯分类方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 因为算法有设置自动停止 所以步长设置的比较小，迭代次数可以放的多\n",
    "model =  MultinomialNB(alpha=1, fit_prior=True, class_prior=None)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train_vec)\n",
    "\n",
    "# 对得到的向量进行统一的标准化处理\n",
    "x_train_vec = scaler.transform(x_train_vec)\n",
    "x_test_vec = scaler.transform(x_test_vec)\n",
    "#print y_train\n",
    "model.fit(x_train_vec,y_train)\n",
    "pp = model.predict(x_test_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# 展示一些统计结果 验证准确度\n",
    "print(confusion_matrix(y_test,pp))\n",
    "print(classification_report(y_test,pp))\n",
    "\n",
    "\n",
    "score = model.score(x_test_vec, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证敏感性 \n",
    "当换取4个类别后使用原model结果再测试一下准确度和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_model = Doc2Vec.load('docvec')\n",
    "classDict = {\n",
    "                      'soc.religion.christian':1,\n",
    "                         'talk.politics.guns':2,\n",
    "                         'talk.politics.mideast':3,\n",
    "                          'talk.politics.misc':4,\n",
    "                         \n",
    "                           }\n",
    "def choiced_data(params):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i,v in enumerate(params):  \n",
    "        if v[1] in ['soc.religion.christian','talk.politics.guns','talk.politics.mideast','talk.politics.misc']:      \n",
    "            temp = []\n",
    "            temp.append(v[0])\n",
    "            x_train.append(temp)\n",
    "            y_train.append(classDict[v[1]])\n",
    "    return x_train,y_train\n",
    "\n",
    "\n",
    "#获取这四类新闻的文本和label\n",
    "#x_train,y_train = helper.choiced_data(raw_data['20news-bydate-train'])\n",
    "#x_test,y_test = helper.choiced_data(raw_data['20news-bydate-test'])\n",
    "\n",
    "#将文本转化为文档向量\n",
    "x_train_vec = []\n",
    "x_test_vec = []\n",
    "for i in x_train:\n",
    "    #inferred_vector = train_model.infer_vector([token for token in gensim.utils.simple_preprocess(i[0]) if token not in STOPWORDS])\n",
    "    inferred_vector = train_model.infer_vector(gensim.utils.simple_preprocess(i[0]))\n",
    "    x_train_vec.append(inferred_vector)\n",
    "for j in x_test:\n",
    "    #inferred_vector_test = train_model.infer_vector([token for token in gensim.utils.simple_preprocess(j[0]) if token not in STOPWORDS])\n",
    "    inferred_vector_test = train_model.infer_vector(gensim.utils.simple_preprocess(j[0]))\n",
    "    x_test_vec.append(inferred_vector_test)\n",
    "\n",
    "print x_train_vec[0],y_train[0]\n",
    "print x_test_vec[0],y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "ts = TSNE(2)\n",
    "reduced_vecs = ts.fit_transform(np.concatenate((x_test_vec[:376],x_test_vec[376:686],x_test_vec[686:1084],x_test_vec[1084:])))\n",
    "print reduced_vecs.shape\n",
    "for i in range(len(reduced_vecs)):\n",
    "    if i < 376:\n",
    "        color = 'b'\n",
    "    elif i>=376 and i<686:\n",
    "        color = 'r'\n",
    "    elif i>=686 and i<1084:\n",
    "        color = 'g'\n",
    "    elif i>=1084:\n",
    "        color = 'y'\n",
    "    plt.plot(reduced_vecs[i,0],reduced_vecs[i,1],marker='o',color=color,markersize=3)\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 因为算法有设置自动停止 所以步长设置的比较小，迭代次数可以放的多\n",
    "model =  MLPClassifier(hidden_layer_sizes=(50,25),max_iter=2000,random_state=0,verbose=True,alpha=1,learning_rate_init=0.0001)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_vec)\n",
    "\n",
    "# 对得到的向量进行统一的标准化处理\n",
    "x_train_vec = scaler.transform(x_train_vec)\n",
    "x_test_vec = scaler.transform(x_test_vec)\n",
    "#print y_train\n",
    "model.fit(x_train_vec,y_train)\n",
    "pp = model.predict(x_test_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# 展示一些统计结果 验证准确度\n",
    "print(confusion_matrix(y_test,pp))\n",
    "print(classification_report(y_test,pp))\n",
    "\n",
    "\n",
    "score = model.score(x_test_vec, y_test)\n",
    "print score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
